{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of Copy of Experiments.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM64/SLeC7HZDdUF+xMNOxd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wy-go/google-research/blob/master/spreadsheet_coder/experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxlH4IZDv-Zn"
      },
      "source": [
        "# Data\n",
        "Load data from [[UI]](https://console.cloud.google.com/storage/browser/spreadsheet_coder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzH8cW8jR9sY"
      },
      "source": [
        "\n",
        "## Copy files from GCS with gsutil"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJLt_RvlwHA-"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# https://cloud.google.com/resource-manager/docs/creating-managing-projects\n",
        "project_id = 'sheetcoder'\n",
        "!gcloud config set project {project_id}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtF_mSd4ASwl"
      },
      "source": [
        "%cd /content/\n",
        "!ls\n",
        "!rm -rf google-research \n",
        "# Clone the entire repo.\n",
        "!git clone https://github.com/wy-go/google-research.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yoi37DarpK7B"
      },
      "source": [
        "# Install deps\n",
        "!pip install -q -U tensor2tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRy331zlxcJQ"
      },
      "source": [
        "# Download the file from a given Google Cloud Storage bucket.\n",
        "%cd google-research/spreadsheet_coder\n",
        "!gsutil -m cp -r \\\n",
        "  \"gs://spreadsheet_coder/enron/\" .\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4x0uZ6rSB0t"
      },
      "source": [
        "## Read the TFRecord files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElGiJTUW-L3b"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "tf.gfile = tf.io.gfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGDl1T4RDZly"
      },
      "source": [
        "import os\n",
        "\n",
        "data_dir = 'enron/'\n",
        "tfrecord_files = [data_dir + filename for filename in os.listdir(data_dir)]\n",
        "raw_dataset = tf.data.TFRecordDataset(tfrecord_files)\n",
        "\n",
        "# Show the first 3 records.\n",
        "for raw_record in raw_dataset.take(3):\n",
        "  print(repr(raw_record))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uyd7VSL6MANW"
      },
      "source": [
        "# Create a description of the features.\n",
        "feature_description = {\n",
        "    'table_id': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'doc_id': tf.io.FixedLenFeature([], tf.string),\n",
        "    'record_index': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'col_index': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'formula': tf.io.FixedLenFeature([], tf.string),\n",
        "    'formula_token_list': tf.io.FixedLenFeature([], tf.string),\n",
        "    'ranges': tf.io.FixedLenFeature([], tf.string),\n",
        "    'computed_value': tf.io.FixedLenFeature([], tf.string),\n",
        "    'header': tf.io.FixedLenFeature([], tf.string),\n",
        "    'context_header': tf.io.FixedLenFeature([], tf.string),\n",
        "    'context_data': tf.io.FixedLenFeature([], tf.string)\n",
        "}\n",
        "\n",
        "def _parse_example(example_proto):\n",
        "  # Parse the input `tf.train.Example` proto using the dictionary above.\n",
        "  feature_dict = tf.io.parse_single_example(example_proto, feature_description)\n",
        "  return feature_dict\n",
        "\n",
        "parsed_dataset = raw_dataset.map(_parse_example)\n",
        "\n",
        "parsed_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7VRu9ONF1NW"
      },
      "source": [
        "# Show the first 3 observations in datasets.\n",
        "for parsed_record in parsed_dataset.take(10):\n",
        "  print(parsed_record)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCMYiPT93i3b"
      },
      "source": [
        "  <table border=\"1\">\n",
        "  <caption><i>Table 1. </i>Sample Data</caption>\n",
        "    <thead>\n",
        "      <tr>\n",
        "        <th></th>\n",
        "        <th>table_id</th>\n",
        "        <th>doc_id</th>\n",
        "        <th>record_index</th>\n",
        "        <th>col_index</th>\n",
        "        <th>formula</th>\n",
        "        <th>formula_token_list</th>\n",
        "        <th>ranges</th>\n",
        "        <th>computed_value</th>\n",
        "      </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "      <tr align=center>\n",
        "        <th align=left>1</th>\n",
        "        <td>2</td>\n",
        "        <td>b''</td>\n",
        "        <td>1</td>\n",
        "        <td>19</td>\n",
        "        <td>b'RANGE UPLUS FORMULA_START'</td>\n",
        "        <td>b\"b'= +  RANGE'\"</td>\n",
        "        <td>b'G278457581!R[3]C[13]'</td>\n",
        "        <td>b'doub:83.41666666666667'</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>2</th>\n",
        "        <td>2</td>\n",
        "        <td>b''</td>\n",
        "        <td>1</td>\n",
        "        <td>20</td>\n",
        "        <td>b'RANGE UPLUS FORMULA_START'</td>\n",
        "        <td>b\"b'= +  RANGE'\"</td>\n",
        "        <td>b'G278457581!R[4]C[12]'</td>\n",
        "        <td>b'doub:54.220833333333346'</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>3</th>\n",
        "        <td>2</td>\n",
        "        <td>b''</td>\n",
        "        <td>1</td>\n",
        "        <td>21</td>\n",
        "        <td>b'RANGE UPLUS FORMULA_START'</td>\n",
        "        <td>b\"b'= +  RANGE'\"</td>\n",
        "        <td>b'G278457581!R[3]C[-17]'</td>\n",
        "        <td>b'doub:49.215833333333336'</td>\n",
        "      </tr>\n",
        "    </tbody>\n",
        "  </table>\n",
        "\n",
        "<table border=\"1\">\n",
        "    <thead>\n",
        "      <tr>\n",
        "        <th></th>\n",
        "        <th>header</th>\n",
        "        <th>context_header</th>\n",
        "      </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "      <tr align=center>\n",
        "        <th align=left>1</th>\n",
        "        <td>b'Contract Cost'</td>\n",
        "        <td>b'$C[-10]$New/Used$C[-9]$Delivery Date$C[-8]$Owner$C[-7]$DASH\\nApproval\\nDate$C[-6]$Notice to Proceed Given<br/>$C[-5]$Status of Financing$C[-4]$Controlled By$C[-3]$Originator / Developer$C[-2]$Project Manager$C[-1]$Project Name<br/>$C[0]$Contract Cost$C[1]$Contract Paid to Date\\n(Scheduled)$C[2]$\\nCancel-lation Payment$C[3]$Comments$C[4]$Status Update'</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>2</th>\n",
        "        <td>b'Contract Paid to Date\\n(Scheduled)'</td>\n",
        "        <td>b'$C[-10]$Delivery Date$C[-9]$Owner$C[-8]$DASH\\nApproval\\nDate$C[-7]$Notice to Proceed Given$C[-6]$Status of Financing<br/>$C[-5]$Controlled By$C[-4]$Originator / Developer$C[-3]$Project Manager$C[-2]$Project Name$C[-1]$Contract Cost<br/>$C[0]$Contract Paid to Date\\n(Scheduled)$C[1]$\\nCancel-lation Payment$C[2]$Comments$C[3]$Status Update'</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>3</th>\n",
        "        <td>b'\\nCancel-lation Payment'</td>\n",
        "        <td>b'$C[-10]$Owner$C[-9]$DASH\\nApproval\\nDate$C[-8]$Notice to Proceed Given$C[-7]$Status of Financing$C[-6]$Controlled By<br/>$C[-5]$Originator / Developer$C[-4]$Project Manager$C[-3]$Project Name$C[-2]$Contract Cost$C[-1]$Contract Paid to Date\\n(Scheduled)<br/>$C[0]$\\nCancel-lation Payment$C[1]$Comments$C[2]$Status Update'</td>\n",
        "      </tr>\n",
        "    </tbody>\n",
        "<table>\n",
        "\n",
        "<table border=\"1\">\n",
        "    <thead>\n",
        "      <tr>\n",
        "        <th></th>\n",
        "        <th>context_data</th>\n",
        "      </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "      <tr align=center>\n",
        "        <th align=left>1</th>\n",
        "        <td>b'$R[-1]C[-10]$userEnteredValue=empty$<br/>$R[-1]C[-9]$userEnteredValue=empty$<br/>$R[-1]C[-8]$userEnteredValue=empty$<br/>$R[-1]C[-7]$userEnteredValue=empty$<br/>$R[-1]C[-6]$userEnteredValue=empty$<br/>$R[-1]C[-5]$userEnteredValue=empty$<br/>$R[-1]C[-4]$userEnteredValue=empty$<br/>$R[-1]C[-3]$userEnteredValue=empty$<br/>$R[-1]C[-2]$userEnteredValue=empty$<br/>$R[-1]C[-1]$userEnteredValue=empty$<br/>$R[-1]C[0]$userEnteredValue=empty$<br/>$R[-1]C[1]$userEnteredValue=empty$<br/>$R[-1]C[2]$userEnteredValue=empty$<br/>$R[-1]C[3]$userEnteredValue=empty$<br/>$R[-1]C[4]$userEnteredValue=empty$<br/>$R[0]C[-10]$userEnteredValue=str:New$<br/>$R[0]C[-9]$userEnteredValue=doub:37165.0$<br/>$R[0]C[-8]$userEnteredValue=str:Whitewing$<br/>$R[0]C[-7]$userEnteredValue=str:$4.5MM DASHed$<br/>$R[0]C[-6]$userEnteredValue=str:N$<br/>$R[0]C[-5]$userEnteredValue=str:N/A$<br/>$R[0]C[-4]$userEnteredValue=str:EWS$<br/>$R[0]C[-3]$userEnteredValue=str:John Chappell$<br/>$R[0]C[-2]$userEnteredValue=str:Stephen Heck$<br/>$R[0]C[-1]$userEnteredValue=str:Sale in Process$<br/>$R[0]C[0]$formula=RANGE UPLUS FORMULA_START, formulaTokenList==+ RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[3]C[-17], startTokenIndex=2, workbookRangeId=null}], computedValue=doub:83.41666666666667$<br/>$R[0]C[1]$formula=RANGE UPLUS FORMULA_START, formulaTokenList==+ RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[3]C[13], startTokenIndex=2, workbookRangeId=null}], computedValue=doub:54.220833333333346$<br/>$R[0]C[2]$formula=RANGE UPLUS FORMULA_START, formulaTokenList==+ RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[4]C[12], startTokenIndex=2, workbookRangeId=null}], computedValue=doub:49.215833333333336$<br/>$R[0]C[3]$userEnteredValue=empty$<br/>$R[0]C[4]$userEnteredValue=str:DASH in progress.$<br/>$R[1]C[-10]$userEnteredValue=str:New$<br/>$R[1]C[-9]$userEnteredValue=doub:37196.0$<br/>$R[1]C[-8]$userEnteredValue=str:Whitewing$<br/>$R[1]C[-7]$userEnteredValue=str:$4.5MM DASHed$<br/>$R[1]C[-6]$userEnteredValue=str:N$<br/>$R[1]C[-5]$userEnteredValue=str:N/A$<br/>$R[1]C[-4]$userEnteredValue=str:EWS$<br/>$R[1]C[-3]$userEnteredValue=str:John Chappell$<br/>$R[1]C[-2]$userEnteredValue=str:Stephen Heck$<br/>$R[1]C[-1]$userEnteredValue=str:Sale in Process$<br/>$R[1]C[0]$formula=RANGE UPLUS FORMULA_START, formulaTokenList==+ RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[10]C[-17], startTokenIndex=2, workbookRangeId=null}], computedValue=doub:83.41666666666667$<br/>$R[1]C[1]$formula=RANGE UPLUS FORMULA_START, formulaTokenList==+ RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[10]C[13], startTokenIndex=2, workbookRangeId=null}], computedValue=doub:54.220833333333346$<br/>$R[1]C[2]$formula=RANGE UPLUS FORMULA_START, formulaTokenList==+ RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[11]C[12], startTokenIndex=2, workbookRangeId=null}], computedValue=doub:49.215833333333336$<br/>$R[1]C[3]$userEnteredValue=empty$<br/>$R[1]C[4]$userEnteredValue=str:DASH in progress.$<br/>$R[2]C[-10]$userEnteredValue=str:New$<br/>$R[2]C[-9]$userEnteredValue=doub:37226.0$<br/>$R[2]C[-8]$userEnteredValue=str:Whitewing$<br/>$R[2]C[-7]$userEnteredValue=str:$4.5MM DASHed$<br/>$R[2]C[-6]$userEnteredValue=str:N$<br/>$R[2]C[-5]$userEnteredValue=str:N/A$<br/>$R[2]C[-4]$userEnteredValue=str:EWS$<br/>$R[2]C[-3]$userEnteredValue=str:John Chappell$<br/>$R[2]C[-2]$userEnteredValue=str:Stephen Heck$<br/>$R[2]C[-1]$userEnteredValue=str:Sale in Process$<br/>$R[2]C[0]$formula=RANGE UPLUS FORMULA_START, formulaTokenList==+ RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[17]C[-17], startTokenIndex=2, workbookRangeId=null}], computedValue=doub:83.41666666666667$<br/>$R[2]C[1]$formula=RANGE UPLUS FORMULA_START, formulaTokenList==+ RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[17]C[13], startTokenIndex=2, workbookRangeId=null}], computedValue=doub:54.220833333333346$<br/>$R[2]C[2]$formula=RANGE UPLUS FORMULA_START, formulaTokenList==+ RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[18]C[12], startTokenIndex=2, workbookRangeId=null}], computedValue=doub:49.215833333333336$<br/>$R[2]C[3]$userEnteredValue=empty$<br/>$R[2]C[4]$userEnteredValue=str:DASH in progress.$<br/>$R[3]C[-10]$userEnteredValue=str:New$<br/>$R[3]C[-9]$userEnteredValue=doub:37135.0$<br/>$R[3]C[-8]$userEnteredValue=str:ENE B/S$<br/>$R[3]C[-7]$userEnteredValue=str:Approved$<br/>$R[3]C[-6]$userEnteredValue=str:N$<br/>$R[3]C[-5]$userEnteredValue=str:N/A$<br/>$R[3]C[-4]$userEnteredValue=str:EWS$<br/>$R[3]C[-3]$userEnteredValue=str:Dick Westfahl$<br/>$R[3]C[-2]$userEnteredValue=empty$<br/>$R[3]C[-1]$userEnteredValue=str:NEPCO / NESCO - Goldendale (EECC)$<br/>$R[3]C[0]$formula=RANGE UPLUS FORMULA_START, formulaTokenList==+ RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[32]C[-17], startTokenIndex=2, workbookRangeId=null}], computedValue=doub:36.24736$<br/>$R[3]C[1]$formula=RANGE UPLUS FORMULA_START, formulaTokenList==+ RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[32]C[13], startTokenIndex=2, workbookRangeId=null}], computedValue=doub:35.613031199999995$<br/>$R[3]C[2]$formula=RANGE UPLUS FORMULA_START, formulaTokenList==+ RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[33]C[12], startTokenIndex=2, workbookRangeId=null}], computedValue=doub:35.7036496$<br/>$R[3]C[3]$userEnteredValue=empty$<br/>$R[3]C[4]$userEnteredValue=str:Contract in the works, possible buyer$<br/>$R[4]C[-10]$userEnteredValue=str:Used$<br/>$R[4]C[-9]$userEnteredValue=str:Delivered$<br/>$R[4]C[-8]$userEnteredValue=str:West LB$<br/>$R[4]C[-7]$userEnteredValue=str:Analyzing$<br/>$R[4]C[-6]$userEnteredValue=str:N$<br/>$R[4]C[-5]$userEnteredValue=str:N/A$<br/>$R[4]C[-4]$userEnteredValue=str:EA$<br/>$R[4]C[-3]$userEnteredValue=empty$<br/>$R[4]C[-2]$userEnteredValue=empty$<br/>$R[4]C[-1]$userEnteredValue=str:Sale in Process$<br/>$R[4]C[0]$formula=RANGE UPLUS FORMULA_START, formulaTokenList==+ RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[111]C[-17], startTokenIndex=2, workbookRangeId=null}], computedValue=doub:17.25$<br/>$R[4]C[1]$formula=RANGE UPLUS FORMULA_START, formulaTokenList==+ RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[111]C[13], startTokenIndex=2, workbookRangeId=null}], computedValue=doub:17.25$<br/>$R[4]C[2]$formula=RANGE UPLUS FORMULA_START, formulaTokenList==+ RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[112]C[12], startTokenIndex=2, workbookRangeId=null}], computedValue=doub:17.25$<br/>$R[4]C[3]$userEnteredValue=empty$<br/>$R[4]C[4]$userEnteredValue=str:3 Potential buyers in Canada, no legitimate offer as of 1/25/01.$<br/>$R[5]C[-10]$userEnteredValue=str:Used$<br/>$R[5]C[-9]$userEnteredValue=str:Delivered$<br/>$R[5]C[-8]$userEnteredValue=str:West LB$<br/>$R[5]C[-7]$userEnteredValue=str:Analyzing$<br/>$R[5]C[-6]$userEnteredValue=str:N$<br/>$R[5]C[-5]$userEnteredValue=str:N/A$<br/>$R[5]C[-4]$userEnteredValue=str:EA$<br/>$R[5]C[-3]$userEnteredValue=empty$<br/>$R[5]C[-2]$userEnteredValue=empty$<br/>$R[5]C[-1]$userEnteredValue=str:Sale in Process$<br/>$R[5]C[0]$formula=RANGE UPLUS FORMULA_START, formulaTokenList==+ RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[118]C[-17], startTokenIndex=2, workbookRangeId=null}], computedValue=doub:17.25$<br/>$R[5]C[1]$formula=RANGE UPLUS FORMULA_START, formulaTokenList==+ RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[118]C[13], startTokenIndex=2, workbookRangeId=null}], computedValue=doub:17.25$<br/>$R[5]C[2]$formula=RANGE UPLUS FORMULA_START, formulaTokenList==+ RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[119]C[12], startTokenIndex=2, workbookRangeId=null}], computedValue=doub:17.25$<br/>$R[5]C[3]$userEnteredValue=empty$<br/>$R[5]C[4]$userEnteredValue=str:3 Potential buyers in Canada, no legitimate offer as of 1/25/01.$<br/>$R[6]C[-10]$userEnteredValue=str:New$<br/>$R[6]C[-9]$userEnteredValue=str:being cleaned$<br/>$R[6]C[-8]$userEnteredValue=str:ENE B/S$<br/>$R[6]C[-7]$userEnteredValue=str:Analyzing$<br/>$R[6]C[-6]$userEnteredValue=str:N$<br/>$R[6]C[-5]$userEnteredValue=str:N/A$<br/>$R[6]C[-4]$userEnteredValue=str:EA$<br/>$R[6]C[-3]$userEnteredValue=str:David Fairley, Mathew Gimble$<br/>$R[6]C[-2]$userEnteredValue=empty$<br/>$R[6]C[-1]$userEnteredValue=str:Purchaser Identified$<br/>$R[6]C[0]$formula=RANGE UPLUS FORMULA_START, formulaTokenList==+ RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[21]C[-17], startTokenIndex=2, workbookRangeId=null}], computedValue=doub:24.506$<br/>$R[6]C[1]$formula=RANGE UPLUS FORMULA_START, formulaTokenList==+ RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[21]C[13], startTokenIndex=2, workbookRangeId=null}], computedValue=doub:24.506000000000007$<br/>$R[6]C[2]$formula=RANGE UPLUS FORMULA_START, formulaTokenList==+ RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[22]C[12], startTokenIndex=2, workbookRangeId=null}], computedValue=doub:24.506$<br/>$R[6]C[3]$userEnteredValue=str:CALME purchased turbine from ENA; turbine has not yet cleared customs;  generator incurred salt water damage while unloading$<br/>$R[6]C[4]$userEnteredValue=empty$<br/>$R[7]C[-10]$userEnteredValue=empty$<br/>$R[7]C[-9]$userEnteredValue=empty$<br/>$R[7]C[-8]$userEnteredValue=str:E-Next Generation$<br/>$R[7]C[-7]$userEnteredValue=str:$16.5MM on 2/16/01$<br/>$R[7]C[-6]$userEnteredValue=str:N$<br/>$R[7]C[-5]$userEnteredValue=str:N/A$<br/>$R[7]C[-4]$userEnteredValue=str:EA$<br/>$R[7]C[-3]$userEnteredValue=str:Jake Thomas/Laura Wente$<br/>$R[7]C[-2]$userEnteredValue=empty$<br/>$R[7]C[-1]$userEnteredValue=str:Columbia$<br/>$R[7]C[0]$formula=RANGE FORMULA_START, formulaTokenList== RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[100]C[-17], startTokenIndex=1, workbookRangeId=null}], computedValue=doub:39.2$<br/>$R[7]C[1]$formula=RANGE FORMULA_START, formulaTokenList== RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[100]C[13], startTokenIndex=1, workbookRangeId=null}], computedValue=doub:6.272$<br/>$R[7]C[2]$formula=RANGE FORMULA_START, formulaTokenList== RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[101]C[12], startTokenIndex=1, workbookRangeId=null}], computedValue=doub:4.704$<br/>$R[7]C[3]$userEnteredValue=empty$<br/>$R[7]C[4]$userEnteredValue=empty$<br/>$R[8]C[-10]$userEnteredValue=str:New$<br/>$R[8]C[-9]$userEnteredValue=doub:37408.0$<br/>$R[8]C[-8]$userEnteredValue=str:West LB$<br/>$R[8]C[-7]$userEnteredValue=str:$2.5MM on 1/31/01$<br/>$R[8]C[-6]$userEnteredValue=str:N$<br/>$R[8]C[-5]$userEnteredValue=str:N/A$<br/>$R[8]C[-4]$userEnteredValue=str:EA$<br/>$R[8]C[-3]$userEnteredValue=empty$<br/>$R[8]C[-2]$userEnteredValue=empty$<br/>$R[8]C[-1]$userEnteredValue=str:Fort Pierce$<br/>$R[8]C[0]$formula=RANGE UPLUS FORMULA_START, formulaTokenList==+ RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[139]C[-17], startTokenIndex=2, workbookRangeId=null}], computedValue=doub:43.618$<br/>$R[8]C[1]$formula=RANGE UPLUS FORMULA_START, formulaTokenList==+ RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[139]C[13], startTokenIndex=2, workbookRangeId=null}], computedValue=doub:34.894400000000005$<br/>$R[8]C[2]$formula=RANGE UPLUS FORMULA_START, formulaTokenList==+ RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[140]C[12], startTokenIndex=2, workbookRangeId=null}], computedValue=doub:43.618$<br/>$R[8]C[3]$userEnteredValue=empty$<br/>$R[8]C[4]$userEnteredValue=empty$<br/>$R[9]C[-10]$userEnteredValue=str:New$<br/>$R[9]C[-9]$userEnteredValue=doub:37591.0$<br/>$R[9]C[-8]$userEnteredValue=empty$<br/>$R[9]C[-7]$userEnteredValue=str:$4.2MM on 7/24/00$<br/>$R[9]C[-6]$userEnteredValue=str:N$<br/>$R[9]C[-5]$userEnteredValue=str:N/A$<br/>$R[9]C[-4]$userEnteredValue=str:EA$<br/>$R[9]C[-3]$userEnteredValue=str:Maurice Gilbert$<br/>$R[9]C[-2]$userEnteredValue=empty$<br/>$R[9]C[-1]$userEnteredValue=str:Las Vegas CoGen II$<br/>$R[9]C[0]$formula=RANGE UPLUS FORMULA_START, formulaTokenList==+ RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[34]C[-17], startTokenIndex=2, workbookRangeId=null}], computedValue=doub:15.769725$<br/>$R[9]C[1]$formula=RANGE UPLUS FORMULA_START, formulaTokenList==+ RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[34]C[13], startTokenIndex=2, workbookRangeId=null}], computedValue=doub:3.94243125$<br/>$R[9]C[2]$formula=RANGE UPLUS FORMULA_START, formulaTokenList==+ RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[35]C[12], startTokenIndex=2, workbookRangeId=null}], computedValue=doub:3.153945$<br/>$R[9]C[3]$userEnteredValue=empty$<br/>$R[9]C[4]$userEnteredValue=empty$<br/>$R[10]C[-10]$userEnteredValue=str:New$<br/>$R[10]C[-9]$userEnteredValue=doub:37257.0$<br/>$R[10]C[-8]$userEnteredValue=empty$<br/>$R[10]C[-7]$userEnteredValue=str:$4.2MM on 7/24/00$<br/>$R[10]C[-6]$userEnteredValue=str:N$<br/>$R[10]C[-5]$userEnteredValue=str:N/A$<br/>$R[10]C[-4]$userEnteredValue=str:EA$<br/>$R[10]C[-3]$userEnteredValue=str:Maurice Gilbert$<br/>$R[10]C[-2]$userEnteredValue=empty$<br/>$R[10]C[-1]$userEnteredValue=str:Las Vegas CoGen II$<br/>$R[10]C[0]$formula=RANGE UPLUS FORMULA_START, formulaTokenList==+ RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[41]C[-17], startTokenIndex=2, workbookRangeId=null}], computedValue=doub:15.769725$<br/>$R[10]C[1]$formula=RANGE UPLUS FORMULA_START, formulaTokenList==+ RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[41]C[13], startTokenIndex=2, workbookRangeId=null}], computedValue=doub:3.94243125$<br/>$R[10]C[2]$formula=RANGE UPLUS FORMULA_START, formulaTokenList==+ RANGE, ranges=[R1C1FormulaRange{relativeRange=G278457581!R[42]C[12], startTokenIndex=2, workbookRangeId=null}], computedValue=doub:3.153945$<br/>$R[10]C[3]$userEnteredValue=empty$<br/>$R[10]C[4]$userEnteredValue=empty$'\n",
        "      </tr>\n",
        "    </tbody>\n",
        "<table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x382mge6cehq"
      },
      "source": [
        "## Data preprocess\n",
        "```\n",
        "record_index: tf.int64\n",
        "col_index: tf.int64\n",
        "formula: tf.string, '<SKETCH> $ENDSKETCH$ $R$ R[]C[] $RANGESPLIT$ R[]C[] $ENDFORMULASKETCH$ EOF'\n",
        "context_header: tf.string\n",
        "context_data: tf.string, [21, 21]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FInrD0zwBHs1"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "from constants import SPECIAL_TOKEN_LIST, END_FORMULA_SKETCH_ID\n",
        "\n",
        "D = 10\n",
        "print(SPECIAL_TOKEN_LIST)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuOosFJnP4kn"
      },
      "source": [
        "# Complete brackets, eliminate Gxxxx\n",
        "def format_range(range_instance):\n",
        "  range_eliminate = re.sub('G\\d*', '', range_instance)\n",
        "  range_eliminate = re.sub('\"', '', range_eliminate)\n",
        "  str_list = list(range_eliminate)\n",
        "  for i in range(len(str_list) + 8):\n",
        "    if i >= len(str_list):\n",
        "      break\n",
        "    if (str_list[i] == 'R' or str_list[i] == 'C') and str_list[i + 1] != '[':\n",
        "      str_list.insert(i + 1, '[')\n",
        "    if (str_list[i] == 'C' or str_list[i] == ':') and str_list[i - 1] != ']':\n",
        "        str_list.insert(i, ']')\n",
        "  if str_list[-1] != ']':\n",
        "    str_list += ']'\n",
        "\n",
        "  format_string = ''.join(str_list)\n",
        "  range_values = np.array([int(range_string[1:-1]) for range_string in re.findall('\\[.*?\\]', format_string)])\n",
        "  exceed = not ((-D <= range_values) & (range_values < 11)).all()\n",
        "\n",
        "  return format_string, exceed\n",
        "\n",
        "a, b = format_range('G88601434R-11C5:R[3]C[9\"')\n",
        "print(a, b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltksuY9HpXOl"
      },
      "source": [
        "#\n",
        "def format_example(example):\n",
        "\n",
        "  formula_token_list = str(example['formula_token_list'].numpy())\n",
        "  ranges = str(example['ranges'].numpy())\n",
        "  context_header = str(example['context_header'].numpy())\n",
        "  context_data = str(example['context_data'].numpy())\n",
        "\n",
        "\n",
        "  sketch = re.search(\"b'(.*)'\", formula_token_list).group(1)\n",
        "  range_simple = re.search(\"b'(.*)'\", ranges)\n",
        "  if range_simple:\n",
        "    range_list = range_simple.group(1).split(' ')\n",
        "  else:\n",
        "    range_list = re.findall('!(.*?) ', ranges) + ranges.split(\"!\")[-1:]\n",
        "  exceed = False\n",
        "  format_ranges = []\n",
        "  for ra in range_list:\n",
        "    format_ra, exceed = format_range(ra)\n",
        "    format_ranges.append(format_ra)\n",
        "    if exceed:\n",
        "      break\n",
        "  if exceed:\n",
        "    return None\n",
        "\n",
        "  formula = sketch + ' $ENDFORMULASKETCH$ $R$ '\n",
        "  for (i, range_i) in enumerate(format_ranges):\n",
        "    if(i):\n",
        "      formula += ' $RANGESPLIT$ '\n",
        "    formula += range_i\n",
        "  formula += ' $ENDRANGE$ EOF'\n",
        "  \n",
        "  tokenizer = tfds.deprecated.text.Tokenizer(\n",
        "      alphanum_only=False, reserved_tokens=SPECIAL_TOKEN_LIST + ['$R$'])\n",
        "  formula_tokens = tokenizer.tokenize(formula.replace(' ', ''))\n",
        "  formula_doc.append(formula_tokens)\n",
        "  \n",
        "  cells = context_header[3:-1].split('$')\n",
        "\n",
        "  if len(cells):\n",
        "    start_col_index = int(cells[0][2:-1])\n",
        "    for i in range(len(cells)):\n",
        "      if i >= len(cells):\n",
        "        break\n",
        "      while i % 2 == 0 and cells[i] != \"C[\" + str(start_col_index + i // 2) + \"]\":\n",
        "        part_of_former = cells[i]\n",
        "        cells.pop(i)\n",
        "        if i > 0:\n",
        "          cells[i - 1] += '$' + part_of_former\n",
        "        if i >= len(cells):\n",
        "          break\n",
        "      if i >= len(cells):\n",
        "        break\n",
        "        \n",
        "    end_col_index = int(cells[-2][2:-1])\n",
        "  else:\n",
        "    start_col_index = 10\n",
        "    end_col_index = 10\n",
        "\n",
        "  context_header = []\n",
        "  if start_col_index > -D:\n",
        "    context_header += ['empty'] * (start_col_index + D)\n",
        "\n",
        "  for i in range(end_col_index - start_col_index + 1):\n",
        "    if cells[2 * i + 1]:\n",
        "      context_header += [cells[2 * i + 1]]\n",
        "    else:\n",
        "      context_header += ['empty']\n",
        "\n",
        "  if end_col_index < D:\n",
        "    context_header += ['empty'] * (D - end_col_index)\n",
        "\n",
        "\n",
        "  cells = context_data[3:-1].split('$')\n",
        "    \n",
        "  context_data = ['empty'] * 21 * 21\n",
        "  cell_num = len(cells) // 3\n",
        "  last_index = 0\n",
        "\n",
        "  for i in range(cell_num):\n",
        "\n",
        "    if len(cells) <= 3 * i:\n",
        "      break;\n",
        "    cell_index = re.search(\"R\\[(.*)\\]C\\[(.*)\\]\", cells[3 * i])\n",
        "    while not cell_index:\n",
        "      part_of_last = cells[3 * i - 1]\n",
        "      cells.pop(3 * i - 1)\n",
        "      context_data[last_index] += '$' + part_of_last\n",
        "      if 3 * i >= len(cells):\n",
        "        break\n",
        "      cell_index = re.search(\"R\\[(.*)\\]C\\[(.*)\\]\", cells[3 * i])\n",
        "    if not cell_index:\n",
        "      break\n",
        "\n",
        "    row_index = int(cell_index.group(1))\n",
        "    col_index = int(cell_index.group(2))\n",
        "    context_index = (row_index + 10) * 21 + col_index + 10\n",
        "    last_index = context_index\n",
        "\n",
        "    value = cells[3 * i + 1]\n",
        "    value = re.search(\"Value=(.*)\", value).group(1)\n",
        "\n",
        "    context_data[context_index] = value\n",
        "\n",
        "  formula = tf.convert_to_tensor(np.array(formula),\n",
        "                                         dtype=tf.string)\n",
        "  context_header = tf.convert_to_tensor(np.array(context_header),\n",
        "                                         dtype=tf.string)\n",
        "  context_data = tf.convert_to_tensor(np.array(context_data).reshape(21, 21),\n",
        "                                         dtype=tf.string)\n",
        "  return {'record_index': example['record_index'], \n",
        "          'col_index': example['col_index'],\n",
        "          'formula': formula, \n",
        "          'context_header': context_header,\n",
        "          'context_data': context_data\n",
        "          }\n",
        "\n",
        "format_dataset = []\n",
        "formula_doc = []\n",
        "ORIG_SIZE = 0\n",
        "for parsed_data in parsed_dataset:\n",
        "  format_data = format_example(parsed_data)\n",
        "  ORIG_SIZE +=1\n",
        "  if format_data:\n",
        "    format_dataset.append(format_data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLCH-czFQ1nX"
      },
      "source": [
        "# Show observations in datasets.\n",
        "for format_record in format_dataset[1:2]:\n",
        "  print(repr(format_record))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLaABr7OcB4k"
      },
      "source": [
        "## Construct output vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwoC3LHghvTP"
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "class Vocab:\n",
        "  def __init__(self, tokens=None):\n",
        "    self.idx_to_token = list()\n",
        "    self.token_to_idx = dict()\n",
        "\n",
        "    if tokens is not None:\n",
        "      if \"UNK\" not in tokens:\n",
        "        tokens = tokens + [\"UNK\"]\n",
        "      for token in tokens:\n",
        "        self.idx_to_token.append(token)\n",
        "        self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
        "      self.unk = self.token_to_idx['UNK']\n",
        "\n",
        "  @classmethod\n",
        "  def build(cls, text, min_freq=1, reserved_tokens=None):\n",
        "    token_freqs = defaultdict(int)\n",
        "    for sentence in text:\n",
        "      for token in sentence:\n",
        "        token_freqs[token] += 1\n",
        "    token_list = reserved_tokens if reserved_tokens else []\n",
        "    uniq_tokens = set(token_list)\n",
        "    for token, freq in token_freqs.items():\n",
        "      if freq >= min_freq and (token not in uniq_tokens):\n",
        "        token_list += [token]\n",
        "        uniq_tokens.add(token)\n",
        "    return cls(tokens=token_list)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.idx_to_token)\n",
        "\n",
        "  def __getitem__(self, token):\n",
        "    return self.token_to_idx.get(token, self.unk)\n",
        "  \n",
        "  def convert_tokens_to_ids(self, tokens):\n",
        "    return [self[token] for token in tokens]\n",
        "\n",
        "  def convert_ids_to_tokens(self, indices):\n",
        "    return [self.idx_to_token[index] for index in indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOImtGCliI38"
      },
      "source": [
        "# Filter out tokens that appear less than 10 times in the training set to construct the output formula token vocabulary.\n",
        "reserver_tokens = list(SPECIAL_TOKEN_LIST) + ['$R$']\n",
        "output_vocab = Vocab.build(formula_doc, min_freq=10, reserved_tokens=reserver_tokens)\n",
        "VOCAB_SIZE = len(output_vocab)\n",
        "\n",
        "print(output_vocab.idx_to_token)\n",
        "print(VOCAB_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCuhZLlk-AZ4"
      },
      "source": [
        "\n",
        "  <br />\n",
        "  \n",
        "  <table border=\"1\">\n",
        "  <caption><i>Table 1. </i>Spreadsheet Functions & Operators</caption>\n",
        "    <thead>\n",
        "      <tr>\n",
        "        <th></th>\n",
        "        <th>Spreadsheet Functions</th>\n",
        "        <th>Type</th>\n",
        "      </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "      <tr align=center>\n",
        "        <th align=left>1</th>\n",
        "        <td>ADD(+)</td>\n",
        "        <td>Operator</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>2</th>\n",
        "        <td>MINUS(-)</td>\n",
        "        <td>Operator</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>3</th>\n",
        "        <td>MULPTIPLY(*)</td>\n",
        "        <td>Operator</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>4</th>\n",
        "        <td>DIV(/)</td>\n",
        "        <td>Operator</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>5</th>\n",
        "        <td>UPLUS</td>\n",
        "        <td>Operator</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>6</th>\n",
        "        <td>UMINUS</td>\n",
        "        <td>Operator</td>\n",
        "      </tr>      \n",
        "      <tr align=center>\n",
        "        <th align=left>7</th>\n",
        "        <td>SUM</td>\n",
        "        <td>Math</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>8</th>\n",
        "        <td>ABS</td>\n",
        "        <td>Math</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>9</th>\n",
        "        <td>LN</td>\n",
        "        <td>Math</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>10</th>\n",
        "        <td>AVERAGE</td>\n",
        "        <td>Statistical</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>11</th>\n",
        "        <td>MIN</td>\n",
        "        <td>Statistical</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>12</th>\n",
        "        <td>MAX</td>\n",
        "        <td>Statistical</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>13</th>\n",
        "        <td>COUNT</td>\n",
        "        <td>Statistical</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>14</th>\n",
        "        <td>COUNTA</td>\n",
        "        <td>Statistical</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>15</th>\n",
        "        <td>STDEV</td>\n",
        "        <td>Statistical</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>16</th>\n",
        "        <td>DAY</td>\n",
        "        <td>Date</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>17</th>\n",
        "        <td>WEEKDAY</td>\n",
        "        <td>Date</td>\n",
        "      </tr>\n",
        "    </tbody>\n",
        "  </table>\n",
        "\n",
        "<br />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SrRuRgiSLu-"
      },
      "source": [
        "## Statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVOhyAV7hpsK"
      },
      "source": [
        "### From paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKKmfZUIekmw"
      },
      "source": [
        "  <table border=\"1\">\n",
        "  <caption><i>Table 2. </i>Dataset</caption>\n",
        "    <thead>\n",
        "      <tr>\n",
        "        <th></th>\n",
        "        <th>Train</th>\n",
        "        <th>Validation</th>\n",
        "        <th>Test</th>\n",
        "        <th>Total</th>\n",
        "      </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "      <tr align=center>\n",
        "        <th align=left>#Samples</th>\n",
        "        <td>178K</td>\n",
        "        <td>41K</td>\n",
        "        <td>33K</td>\n",
        "        <td>252K</td>\n",
        "      </tr>\n",
        "    </tbody>\n",
        "  </table>\n",
        "\n",
        "  <br />\n",
        "\n",
        "  <table border=\"1\">\n",
        "  <caption><i>Table 3. </i>Sketch Length (excluding ENDSKETCH).</caption>\n",
        "    <thead>\n",
        "      <tr>\n",
        "        <th align=left>Length</th>\n",
        "        <th>2</th>\n",
        "        <th>3</th>\n",
        "        <th>4-5</th>\n",
        "        <th>6-7</th>\n",
        "        <th>8+</th>\n",
        "      </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "      <tr align=center>\n",
        "        <th align=left>Distribution</th>\n",
        "        <td>55%</td>\n",
        "        <td>18%</td>\n",
        "        <td>13%</td>\n",
        "        <td>9%</td>\n",
        "        <td>5%</td>\n",
        "      </tr>\n",
        "    </tbody>\n",
        "  </table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeAMF31Nco9M"
      },
      "source": [
        "### From data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8V676uLbITt"
      },
      "source": [
        "import tokenization\n",
        "\n",
        "TOTAL_SIZE = len(format_dataset)\n",
        "print(\"original dataset size: %d\" %(ORIG_SIZE))\n",
        "print(\"dataset size: %d\" %(TOTAL_SIZE))\n",
        "\n",
        "formula_len = defaultdict(int)\n",
        "sketch_len = defaultdict(int)\n",
        "\n",
        "formula_ids = []\n",
        "for i, data in enumerate(format_dataset):\n",
        "  formula = output_vocab.convert_tokens_to_ids(formula_doc[i])\n",
        "  formula_ids.append(formula)\n",
        "  formula_len[len(formula)] += 1\n",
        "  sketch_len[formula.index(END_FORMULA_SKETCH_ID)] += 1\n",
        "\n",
        "print(\"full formula length distribution:\")\n",
        "print(formula_len)\n",
        "print(\"sketch length distribution (excluding $END_FORMULA_SKETCH$):\")\n",
        "print(sketch_len)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44uu9DiRCNdK"
      },
      "source": [
        "# Observe\n",
        "for i in [10, 210, 350]:\n",
        "  print(formula_doc[i], formula_ids[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoF_EwC-a8PE"
      },
      "source": [
        "## Split dataset and save as TFRecord"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrwDr_05-Npd"
      },
      "source": [
        "VOCAB_FILE = 'vocab.txt'\n",
        "FORMULA_LENGTH = 100 # not provided in paper\n",
        "MAX_CELL_CONTEXT_LENGTH = 128\n",
        "\n",
        "TRAIN_SIZE = 130000\n",
        "VALID_SIZE = 25000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e9RYqAZTQvL"
      },
      "source": [
        "def _truncate_cell_index(len_list, max_len):\n",
        "  st_idx, end_idx = (0, 21)\n",
        "  total_len = sum(len_list)\n",
        "  while total_len > max_len:\n",
        "    if end_idx - 11 == 10 - st_idx:\n",
        "      end_idx -= 1\n",
        "      total_len -= len_list[end_idx]\n",
        "    else:\n",
        "      st_idx += 1\n",
        "      total_len -= len_list[st_idx - 1]\n",
        "  return st_idx, end_idx\n",
        "\n",
        "def row_context_builder(context_data, context_header):\n",
        "  if not isinstance(context_data, list):\n",
        "    context_data = context_data.tolist()\n",
        "    context_header = context_header.tolist()\n",
        "  context_data = [context_header] + context_data\n",
        "  row_context = []\n",
        "  pad_lens = []\n",
        "  cell_ranges = []\n",
        "  cell_indices = []\n",
        "  for i in range(22):\n",
        "    len_list = []\n",
        "    for j in range(21):\n",
        "      tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=True)\n",
        "      context =  tokenization.convert_to_unicode(context_data[i][j])\n",
        "      tokens = tokenizer.tokenize(context) + [\"[SEP]\"]\n",
        "      tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
        "      len_list.append(len(tokens))\n",
        "      context_data[i][j] = tokens\n",
        "    cur_line = []\n",
        "    st_idx, end_idx = _truncate_cell_index(len_list, MAX_CELL_CONTEXT_LENGTH)\n",
        "    cell_ranges.append((st_idx, end_idx))\n",
        "    cell_indices.append(len_list)\n",
        "    for k in range(st_idx, end_idx):\n",
        "      cur_line += context_data[i][k]\n",
        "    pad_len = MAX_CELL_CONTEXT_LENGTH - len(cur_line)\n",
        "    pad_lens.append(pad_len)\n",
        "    cur_line += [0] * pad_len # [PAD]\n",
        "    row_context += cur_line\n",
        "  return row_context, pad_lens, cell_ranges, cell_indices\n",
        "\n",
        "\n",
        "def col_context_builder(context_data):\n",
        "  transpose = []\n",
        "  for j in range(21):\n",
        "    cur = []\n",
        "    for i in range(21):\n",
        "      cur.append(context_data[i][j])\n",
        "    transpose.append(cur)\n",
        "  col_header = list(transpose[10])\n",
        "  return row_context_builder(transpose, col_header)\n",
        "\n",
        "def mask_builder(pad_lens):\n",
        "  mask = []\n",
        "  for pad_len in pad_lens:\n",
        "    mask += [1.0] * (MAX_CELL_CONTEXT_LENGTH - pad_len) + [0.0] * pad_len\n",
        "  # print(\"mask %d\" % np.array(mask).shape)\n",
        "  return mask\n",
        "\n",
        "def indices_builder(len_list, ranges, type):\n",
        "  indices = []\n",
        "  for i in range(len(ranges)):\n",
        "    st_idx, end_idx = ranges[i]\n",
        "    cur = [0] * st_idx + len_list[i][st_idx:end_idx] + [0] * (21 - end_idx)\n",
        "    if type == \"col\":\n",
        "      cur = [0] + cur\n",
        "    indices += cur\n",
        "  # print(\"indices %d\" % np.array(indices).shape)\n",
        "  return indices\n",
        "\n",
        "def mask_per_cell_builder(cell_ranges, type):\n",
        "  mask = []\n",
        "  for (st_idx, end_idx) in cell_ranges:\n",
        "    cur = [0.0] * st_idx + [1.0] * (end_idx - st_idx) + [0.0] * (21 - end_idx)\n",
        "    if type == \"col\":\n",
        "      cur = [0.0] + cur\n",
        "    mask += cur\n",
        "  # print(\"mask per cell %d\" % np.array(mask).shape)\n",
        "  return mask\n",
        "\n",
        "def pad_formula(formula_id):\n",
        "  formula_pad_len = FORMULA_LENGTH - len(formula_id)\n",
        "  return   formula_id + [0] * formula_pad_len\n",
        "\n",
        "def example2feature(id, data):\n",
        "  formula = tf.convert_to_tensor(pad_formula(formula_ids[id]))\n",
        "  row_context, pad_row, row_cells, row_indices = row_context_builder(data['context_data'].numpy(), data['context_header'].numpy())\n",
        "  col_context, pad_col, col_cells, col_indices = col_context_builder(data['context_data'].numpy())\n",
        "  row_mask = tf.convert_to_tensor(mask_builder(pad_row))\n",
        "  col_mask = tf.convert_to_tensor(mask_builder(pad_col))\n",
        "  segment_ids = [0] * MAX_CELL_CONTEXT_LENGTH + [1] * 21 * MAX_CELL_CONTEXT_LENGTH\n",
        "  row_indices = tf.convert_to_tensor(indices_builder(row_indices, row_cells, \"row\"))\n",
        "  col_indices = tf.convert_to_tensor(indices_builder(col_indices, col_cells, \"col\"))\n",
        "  row_mask_per_cell = tf.convert_to_tensor(mask_per_cell_builder(row_cells, \"row\"))\n",
        "  col_mask_per_cell = tf.convert_to_tensor(mask_per_cell_builder(col_cells, \"col\"))\n",
        "  row_segment_ids_per_cell = tf.convert_to_tensor([0] * 21 + [1] * 21 * 21)\n",
        "  col_segment_ids_per_cell = tf.convert_to_tensor([0] * 22 + [1] * 21 * 22)\n",
        "  record_index = tf.convert_to_tensor([data['record_index'].numpy()])\n",
        "  col_index = tf.convert_to_tensor([data['col_index'].numpy()])\n",
        "\n",
        "  serialized_formula = tf.io.serialize_tensor(formula)\n",
        "  serialized_row_context = tf.io.serialize_tensor(row_context)\n",
        "  serialized_col_context = tf.io.serialize_tensor(col_context)\n",
        "  serialized_row_mask = tf.io.serialize_tensor(row_mask)\n",
        "  serialized_col_mask = tf.io.serialize_tensor(col_mask)\n",
        "  serialized_segment_ids = tf.io.serialize_tensor(segment_ids)\n",
        "  serialized_row_indices = tf.io.serialize_tensor(row_indices)\n",
        "  serialized_col_indices = tf.io.serialize_tensor(col_indices)\n",
        "  serialized_row_mask_per_cell = tf.io.serialize_tensor(row_mask_per_cell)\n",
        "  serialized_col_mask_per_cell = tf.io.serialize_tensor(col_mask_per_cell)\n",
        "  serialized_row_segment_ids_per_cell = tf.io.serialize_tensor(row_segment_ids_per_cell)\n",
        "  serialized_col_segment_ids_per_cell = tf.io.serialize_tensor(col_segment_ids_per_cell)\n",
        "  serialized_record_index = tf.io.serialize_tensor(record_index)\n",
        "  serialized_col_index = tf.io.serialize_tensor(col_index)\n",
        "\n",
        "  feature = {\n",
        "          'formula': tf.train.Feature(bytes_list=tf.train.BytesList(value=[serialized_formula.numpy()])),\n",
        "          'row_cell_context': tf.train.Feature(bytes_list=tf.train.BytesList(value=[serialized_row_context.numpy()])),\n",
        "          'col_cell_context': tf.train.Feature(bytes_list=tf.train.BytesList(value=[serialized_col_context.numpy()])),\n",
        "          'row_context_mask': tf.train.Feature(bytes_list=tf.train.BytesList(value=[serialized_row_mask.numpy()])),\n",
        "          'col_context_mask': tf.train.Feature(bytes_list=tf.train.BytesList(value=[serialized_col_mask.numpy()])),\n",
        "          'row_context_segment_ids': tf.train.Feature(bytes_list=tf.train.BytesList(value=[serialized_segment_ids.numpy()])),\n",
        "          'col_context_segment_ids': tf.train.Feature(bytes_list=tf.train.BytesList(value=[serialized_segment_ids.numpy()])),\n",
        "          'row_cell_indices': tf.train.Feature(bytes_list=tf.train.BytesList(value=[serialized_row_indices.numpy()])),\n",
        "          'col_cell_indices': tf.train.Feature(bytes_list=tf.train.BytesList(value=[serialized_col_indices.numpy()])),\n",
        "          'row_context_mask_per_cell': tf.train.Feature(bytes_list=tf.train.BytesList(value=[serialized_row_mask_per_cell.numpy()])),\n",
        "          'col_context_mask_per_cell': tf.train.Feature(bytes_list=tf.train.BytesList(value=[serialized_col_mask_per_cell.numpy()])),\n",
        "          'row_context_segment_ids_per_cell': tf.train.Feature(bytes_list=tf.train.BytesList(value=[serialized_row_segment_ids_per_cell.numpy()])),\n",
        "          'col_context_segment_ids_per_cell': tf.train.Feature(bytes_list=tf.train.BytesList(value=[serialized_col_segment_ids_per_cell.numpy()])),\n",
        "          'record_index': tf.train.Feature(bytes_list=tf.train.BytesList(value=[serialized_record_index.numpy()])),\n",
        "          'col_index': tf.train.Feature(bytes_list=tf.train.BytesList(value=[serialized_col_index.numpy()]))\n",
        "          }\n",
        "  return feature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fx9NhZTLjYL4"
      },
      "source": [
        "!ls\n",
        "!rm -rf datasets\n",
        "!mkdir datasets\n",
        "!touch datasets/train.tfrecords\n",
        "!touch datasets/valid.tfrecords\n",
        "!touch datasets/test.tfrecords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEaqQeexbLmz"
      },
      "source": [
        "data_dir = 'datasets'\n",
        "train_tfrecord = data_dir + '/train.tfrecords'\n",
        "valid_tfrecord = data_dir + '/valid.tfrecords'\n",
        "test_tfrecord = data_dir + '/test.tfrecords'\n",
        "\n",
        "def record_data(file_name, st_idx, end_idx):\n",
        "  with tf.io.TFRecordWriter(file_name) as writer:\n",
        "    for i, data in enumerate(format_dataset[st_idx:end_idx]):\n",
        "      feature = example2feature(i + st_idx, data)\n",
        "      example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "      writer.write(example.SerializeToString())\n",
        "      \n",
        "\n",
        "record_data(train_tfrecord, 0, TRAIN_SIZE)\n",
        "record_data(valid_tfrecord, TRAIN_SIZE, TRAIN_SIZE + VALID_SIZE)\n",
        "record_data(test_tfrecord, TRAIN_SIZE + VALID_SIZE, TOTAL_SIZE)\n",
        "# record_data(train_tfrecord, 0, 1)\n",
        "# record_data(valid_tfrecord, 1, 2)\n",
        "# record_data(test_tfrecord, 2, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNB0t6DN_Fbi"
      },
      "source": [
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "feature_description = {\n",
        "    'formula': tf.io.FixedLenFeature([], tf.string),\n",
        "    'row_cell_context': tf.io.FixedLenFeature([], tf.string),\n",
        "    'col_cell_context': tf.io.FixedLenFeature([], tf.string),\n",
        "    'row_context_mask': tf.io.FixedLenFeature([], tf.string),\n",
        "    'col_context_mask': tf.io.FixedLenFeature([], tf.string),\n",
        "    'row_context_segment_ids': tf.io.FixedLenFeature([], tf.string),\n",
        "    'col_context_segment_ids': tf.io.FixedLenFeature([], tf.string),\n",
        "    'row_cell_indices': tf.io.FixedLenFeature([], tf.string),\n",
        "    'col_cell_indices': tf.io.FixedLenFeature([], tf.string),\n",
        "    'row_context_mask_per_cell': tf.io.FixedLenFeature([], tf.string),\n",
        "    'col_context_mask_per_cell': tf.io.FixedLenFeature([], tf.string),\n",
        "    'row_context_segment_ids_per_cell': tf.io.FixedLenFeature([], tf.string),\n",
        "    'col_context_segment_ids_per_cell': tf.io.FixedLenFeature([], tf.string),\n",
        "    'record_index': tf.io.FixedLenFeature([], tf.string),\n",
        "    'col_index': tf.io.FixedLenFeature([], tf.string),\n",
        "}\n",
        "\n",
        "train_raw = tf.data.TFRecordDataset(train_tfrecord)\n",
        "valid_raw = tf.data.TFRecordDataset(valid_tfrecord)\n",
        "test_raw = tf.data.TFRecordDataset(test_tfrecord)\n",
        "\n",
        "def _parse_example(example_proto):\n",
        "  # Parse the input `tf.train.Example` proto using the dictionary above.\n",
        "  feature_dict = tf.io.parse_single_example(example_proto, feature_description)\n",
        "  feature_dict['formula'] = tf.io.parse_tensor(feature_dict['formula'], tf.int32)\n",
        "  feature_dict['row_cell_context'] = tf.io.parse_tensor(feature_dict['row_cell_context'], tf.int32)\n",
        "  feature_dict['col_cell_context'] = tf.io.parse_tensor(feature_dict['col_cell_context'], tf.int32)\n",
        "  feature_dict['row_context_mask'] = tf.io.parse_tensor(feature_dict['row_context_mask'], tf.float32)\n",
        "  feature_dict['col_context_mask'] = tf.io.parse_tensor(feature_dict['col_context_mask'], tf.float32)\n",
        "  feature_dict['row_context_segment_ids'] = tf.io.parse_tensor(feature_dict['row_context_segment_ids'], tf.int32)\n",
        "  feature_dict['col_context_segment_ids'] = tf.io.parse_tensor(feature_dict['col_context_segment_ids'], tf.int32)\n",
        "  feature_dict['row_cell_indices'] = tf.io.parse_tensor(feature_dict['row_cell_indices'], tf.int32)\n",
        "  feature_dict['col_cell_indices'] = tf.io.parse_tensor(feature_dict['col_cell_indices'], tf.int32)\n",
        "  feature_dict['row_context_mask_per_cell'] = tf.io.parse_tensor(feature_dict['row_context_mask_per_cell'], tf.float32)\n",
        "  feature_dict['col_context_mask_per_cell'] = tf.io.parse_tensor(feature_dict['col_context_mask_per_cell'], tf.float32)\n",
        "  feature_dict['row_context_segment_ids_per_cell'] = tf.io.parse_tensor(feature_dict['row_context_segment_ids_per_cell'], tf.int32)\n",
        "  feature_dict['col_context_segment_ids_per_cell'] = tf.io.parse_tensor(feature_dict['col_context_segment_ids_per_cell'], tf.int32)\n",
        "  feature_dict['record_index'] = tf.io.parse_tensor(feature_dict['record_index'], tf.int32)\n",
        "  feature_dict['col_index'] = tf.io.parse_tensor(feature_dict['col_index'], tf.int32)\n",
        "  return feature_dict\n",
        "\n",
        "train_data = train_raw.map(_parse_example)\n",
        "valid_data = valid_raw.map(_parse_example)\n",
        "test_data = test_raw.map(_parse_example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vXRwV5rRGtf"
      },
      "source": [
        "train_data = train_data.shuffle(buffer_size=2000)\n",
        "train_data = train_data.repeat()\n",
        "train_data = train_data.batch(BATCH_SIZE)\n",
        "train_data = train_data.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "valid_data = valid_data.batch(BATCH_SIZE)\n",
        "\n",
        "test_data = test_data.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7W0qiukRsxo"
      },
      "source": [
        "# Model![model architecture](https://raw.github.com/wy-go/google-research/master/spreadsheet_coder/spreadsheetcoder_model_architecture.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gPHChGOqJ68"
      },
      "source": [
        "bert_modeling:\n",
        "- get_shape_list()\n",
        "- BertModel()\n",
        "- gelu()\n",
        "\n",
        "model_utils:\n",
        "- configure_tpu(flags): Configures the TPU from the command line flags.\n",
        "- class AdamWeightDecayOptimizer(): apply_gradients(grads_and_vars, global_step=None, name=None)\n",
        "- get_train_op(flags, total_loss, ema=None, tvars=None): Generates the training operation.\n",
        "- construct_scalar_host_call(monitor_dict, model_dir, prefix=\"\", reduce_fn=None): Construct host calls to monitor training progress on TPUs.\n",
        "- build_lstm(num_units)\n",
        "- print_tensors(**tensors): Host call function to print Tensors from the TPU during training.\n",
        "- get_assignment_map_from_checkpoint(vars_to_restore, init_checkpoint, bert_prefix=\"\"): Compute the union of the current variables and checkpoint variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MErAi06xh5z9"
      },
      "source": [
        "  <table border=\"1\">\n",
        "    <caption><i>Table 4. </i>Hyper-parameters</caption>\n",
        "    <thead>\n",
        "      <tr align=center>\n",
        "        <th></th>\n",
        "        <th></th>\n",
        "      </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "      <tr align=center>\n",
        "        <th align=left>#Encoder layer</th>\n",
        "        <td></td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>#Decoder layer</th>\n",
        "        <td>1</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>Embedding size</th>\n",
        "        <td></td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>Hidden size</th>\n",
        "        <td>512</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>Dropout rate</th>\n",
        "        <td>0.1</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>Initial lr</th>\n",
        "        <td>5e-5</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>Batch size</th>\n",
        "        <td>64</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>Gradient clipping norm</th>\n",
        "        <td>1.0</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>Optimizer</th>\n",
        "        <td>Adam</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>#Minibatch updates</th>\n",
        "        <td>200K</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>Beam size</th>\n",
        "        <td>64</td>\n",
        "      </tr>\n",
        "    </tbody>\n",
        "  </table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AioSG3_jer67"
      },
      "source": [
        "from model import create_model\n",
        "from model_utils import get_train_op\n",
        "from bert_modeling import BertConfig\n",
        "\n",
        "NUM_ENCODER_LAYERS = 1 # not provided in paper\n",
        "NUM_DECODER_LAYERS = 1 \n",
        "EMBEDDING_SIZE = 512 # not provided in paper\n",
        "HIDDEN_SIZE = 512\n",
        "DROPOUT_RATE = 0.1\n",
        "BEAM_SIZE = 64\n",
        "\n",
        "BERT_CONFIG = BertConfig(hidden_size=512, hidden_act='gelu', \n",
        "                         initializer_range=0.02, vocab_size=30522, \n",
        "                         hidden_dropout_prob=0.1, num_attention_heads=8,\n",
        "                         type_vocab_size=2, max_position_embeddings=512,\n",
        "                         num_hidden_layers=8, intermediate_size=2048,\n",
        "                         attention_probs_dropout_prob=0.1)  # BERT-Medium\n",
        "\n",
        "formula_placeholder = tf.compat.v1.placeholder(name='formula', shape=(BATCH_SIZE, FORMULA_LENGTH), dtype=tf.int32)\n",
        "row_cell_context_placeholder = tf.compat.v1.placeholder(name='row_cell_context', shape=(BATCH_SIZE, 22*MAX_CELL_CONTEXT_LENGTH), dtype=tf.int32) \n",
        "col_cell_context_placeholder = tf.compat.v1.placeholder(name='col_cell_context', shape=(BATCH_SIZE, 22*MAX_CELL_CONTEXT_LENGTH), dtype=tf.int32) \n",
        "row_context_mask_placeholder = tf.compat.v1.placeholder(name='row_context_mask', shape=(BATCH_SIZE, 22*MAX_CELL_CONTEXT_LENGTH), dtype=tf.float32) \n",
        "col_context_mask_placeholder = tf.compat.v1.placeholder(name='col_context_mask', shape=(BATCH_SIZE, 22*MAX_CELL_CONTEXT_LENGTH), dtype=tf.float32) \n",
        "row_context_segment_ids_placeholder = tf.compat.v1.placeholder(name='row_context_segment_ids', shape=(BATCH_SIZE, 22*MAX_CELL_CONTEXT_LENGTH), dtype=tf.int32) \n",
        "col_context_segment_ids_placeholder = tf.compat.v1.placeholder(name='col_context_segment_ids', shape=(BATCH_SIZE, 22*MAX_CELL_CONTEXT_LENGTH), dtype=tf.int32) \n",
        "row_cell_indices_placeholder = tf.compat.v1.placeholder(name='row_cell_indices', shape=(BATCH_SIZE, 22*21), dtype=tf.int32) \n",
        "col_cell_indices_placeholder = tf.compat.v1.placeholder(name='col_cell_indices', shape=(BATCH_SIZE, 22*22), dtype=tf.int32) \n",
        "row_context_mask_per_cell_placeholder = tf.compat.v1.placeholder(name='row_context_mask_per_cell', shape=(BATCH_SIZE, 22*21), dtype=tf.float32) \n",
        "col_context_mask_per_cell_placeholder = tf.compat.v1.placeholder(name='col_context_mask_per_cell', shape=(BATCH_SIZE, 22*22), dtype=tf.float32) \n",
        "row_context_segment_ids_per_cell_placeholder = tf.compat.v1.placeholder(name='row_context_segment_ids_per_cell', shape=(BATCH_SIZE, 22*21), dtype=tf.int32) \n",
        "col_context_segment_ids_per_cell_placeholder = tf.compat.v1.placeholder(name='col_context_segment_ids_per_cell', shape=(BATCH_SIZE, 22*22), dtype=tf.int32) \n",
        "record_index_placeholder = tf.compat.v1.placeholder(name='record_index',shape=(BATCH_SIZE, 1), dtype=tf.int32) \n",
        "col_index_placeholder = tf.compat.v1.placeholder(name='col_index',shape=(BATCH_SIZE, 1), dtype=tf.int32) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5XcC0UlI9ND"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyfpFfm0kyTL"
      },
      "source": [
        "## Full model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruqa4t-uxt9b"
      },
      "source": [
        "y_train = create_model(num_encoder_layers=NUM_ENCODER_LAYERS,\n",
        "                      num_decoder_layers=NUM_DECODER_LAYERS,\n",
        "                      embedding_size=EMBEDDING_SIZE,\n",
        "                      hidden_size=HIDDEN_SIZE,\n",
        "                      dropout_rate=DROPOUT_RATE,\n",
        "                      is_training=True,\n",
        "                      formula=formula_placeholder, \n",
        "                      row_cell_context=row_cell_context_placeholder,\n",
        "                      col_cell_context=col_cell_context_placeholder,\n",
        "                      row_context_mask=row_context_mask_placeholder, \n",
        "                      col_context_mask=col_context_mask_placeholder,\n",
        "                      row_context_segment_ids=row_context_segment_ids_placeholder,\n",
        "                      col_context_segment_ids=col_context_segment_ids_placeholder,\n",
        "                      row_cell_indices=row_cell_indices_placeholder,\n",
        "                      col_cell_indices=col_cell_indices_placeholder,\n",
        "                      row_context_mask_per_cell=row_context_mask_per_cell_placeholder,\n",
        "                      col_context_mask_per_cell=col_context_mask_per_cell_placeholder,\n",
        "                      row_context_segment_ids_per_cell=row_context_segment_ids_per_cell_placeholder,\n",
        "                      col_context_segment_ids_per_cell=col_context_segment_ids_per_cell_placeholder,\n",
        "                      exclude_headers=False,\n",
        "                      max_cell_context_length=MAX_CELL_CONTEXT_LENGTH,\n",
        "                      num_rows=21,\n",
        "                      record_index=record_index_placeholder,\n",
        "                      column_index=col_index_placeholder,\n",
        "                      layer_norm=True,\n",
        "                      cell_position_encoding=True,\n",
        "                      cell_context_encoding=True,\n",
        "                      use_bert=True,\n",
        "                      use_mobilebert=False,\n",
        "                      per_row_encoding=False,\n",
        "                      max_pooling=True, #\n",
        "                      use_cnn=True,\n",
        "                      use_pointer_network=False, #\n",
        "                      two_stage_decoding=True,\n",
        "                      conv_type='cross',\n",
        "                      grid_type='both',\n",
        "                      skip_connection=True,\n",
        "                      bert_config=BERT_CONFIG,\n",
        "                      unused_tensors_to_print=True, # unused\n",
        "                      formula_length=FORMULA_LENGTH,\n",
        "                      formula_prefix_length=0,\n",
        "                      vocab_size=VOCAB_SIZE,\n",
        "                      beam_size=1,\n",
        "                      use_tpu=True,\n",
        "                      use_one_hot_embeddings=False\n",
        "                      )\n",
        "\n",
        "\n",
        "y_test, y_p = create_model(num_encoder_layers=NUM_ENCODER_LAYERS,\n",
        "                           num_decoder_layers=NUM_DECODER_LAYERS,\n",
        "                           embedding_size=EMBEDDING_SIZE,\n",
        "                           hidden_size=HIDDEN_SIZE,\n",
        "                           dropout_rate=DROPOUT_RATE,\n",
        "                           is_training=False,\n",
        "                           formula=formula_placeholder, \n",
        "                           row_cell_context=row_cell_context_placeholder,\n",
        "                           col_cell_context=col_cell_context_placeholder,\n",
        "                           row_context_mask=row_context_mask_placeholder, \n",
        "                           col_context_mask=col_context_mask_placeholder,\n",
        "                           row_context_segment_ids=row_context_segment_ids_placeholder,\n",
        "                           col_context_segment_ids=col_context_segment_ids_placeholder,\n",
        "                           row_cell_indices=row_cell_indices_placeholder,\n",
        "                           col_cell_indices=col_cell_indices_placeholder,\n",
        "                           row_context_mask_per_cell=row_context_mask_per_cell_placeholder,\n",
        "                           col_context_mask_per_cell=col_context_mask_per_cell_placeholder,\n",
        "                           row_context_segment_ids_per_cell=row_context_segment_ids_per_cell_placeholder,\n",
        "                           col_context_segment_ids_per_cell=col_context_segment_ids_per_cell_placeholder,\n",
        "                           exclude_headers=False,\n",
        "                           max_cell_context_length=MAX_CELL_CONTEXT_LENGTH,\n",
        "                           num_rows=21,\n",
        "                           record_index=record_index_placeholder,\n",
        "                           column_index=col_index_placeholder,\n",
        "                           layer_norm=True,\n",
        "                           cell_position_encoding=True,\n",
        "                           cell_context_encoding=True,\n",
        "                           use_bert=True,\n",
        "                           use_mobilebert=False,\n",
        "                           per_row_encoding=False,\n",
        "                           max_pooling=True, #\n",
        "                           use_cnn=True,\n",
        "                           use_pointer_network=False,\n",
        "                           two_stage_decoding=True,\n",
        "                           conv_type='cross',\n",
        "                           grid_type='both',\n",
        "                           skip_connection=True,\n",
        "                           bert_config=BERT_CONFIG,\n",
        "                           unused_tensors_to_print=True, # unused\n",
        "                           formula_length=FORMULA_LENGTH,\n",
        "                           formula_prefix_length=0,\n",
        "                           vocab_size=VOCAB_SIZE,\n",
        "                           beam_size=BEAM_SIZE,\n",
        "                           use_tpu=True,\n",
        "                           use_one_hot_embeddings=False\n",
        "                           )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7gB7qcEvQMI"
      },
      "source": [
        "## Different encoder architecture\n",
        "- — Column-based BERT\n",
        "- — Row-based BERT\n",
        "- — Convolution layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPGYz7R6v9Xn"
      },
      "source": [
        "### — Column-based BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYxX9LtCwJgu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFWIvntQwCST"
      },
      "source": [
        "### — Row-based BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndjXc8KLwHPA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5OPbMFBwH3O"
      },
      "source": [
        "### — Convolution layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gn6C7JGyxx43"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rq4sOOJKwbkz"
      },
      "source": [
        "## Different decoding architecture\n",
        "Same predictor for both the sketch and ranges, with a single output vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpYj_MDxxZTc"
      },
      "source": [
        "### — Two-stage decoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eo1jeC7GwyN9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRVDhZhIwybr"
      },
      "source": [
        "## Different model initialization\n",
        "Randomly initialize BERT encoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av1jJDhhxij2"
      },
      "source": [
        "### — Pretraining"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hYuFFhnwM_F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFE-CsugyDev"
      },
      "source": [
        "## Previous approaches\n",
        "Randomly initialize BERT encoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aT3Tl_PKyUt0"
      },
      "source": [
        "### Row-based RobustFill"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW7UbXeXyeGA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYpkj6dvyeYg"
      },
      "source": [
        "### Column-based RobustFill"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLe-l9dIyiP_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGqFn7G7yocT"
      },
      "source": [
        "## Baseline\n",
        "LSTM decoder only"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIQmX7YfyrU9"
      },
      "source": [
        "### No context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUseNKITy8Y9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU3vlDQnx_sQ"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIhBmCoHGl9F"
      },
      "source": [
        "NUM_BATCHES = 200000\n",
        "\n",
        "WU_STEPS = 1000 #\n",
        "INIT_LR = 5e-5\n",
        "NUM_EPOCH = 72\n",
        "DECAY_METHOD = \"poly\" # or \"cos\"\n",
        "WEIGHT_DECAY = 0.01\n",
        "TRAIN_STEPS = 10000 #\n",
        "MIN_LR_RATIO = 0.1 #\n",
        "ADAM_EPS = 1e-8\n",
        "NUM_CORE_PER_HOST = 1 #\n",
        "CLIP = 1.0\n",
        "\n",
        "\n",
        "class Flags:\n",
        "  def __init__(self, flags_dic={}):\n",
        "    if \"warmup_steps\" in flags_dic.keys():\n",
        "      self.warmup_steps = flags_dic[\"warmup_steps\"]\n",
        "    else:\n",
        "      self.warmup_steps = WU_STEPS\n",
        "    if \"learning_rate\" in flags_dic.keys():\n",
        "      self.learning_rate = flags_dic[\"learning_rate\"]\n",
        "    else:\n",
        "      self.learning_rate = INIT_LR\n",
        "    if \"decay_method\" in flags_dic.keys():\n",
        "      self.decay_method = flags_dic[\"decay_method\"]\n",
        "    else:\n",
        "      self.decay_method = DECAY_METHOD\n",
        "    if \"weight_decay\" in flags_dic.keys():\n",
        "      self.weight_decay = flags_dic[\"weight_decay\"]\n",
        "    else:\n",
        "      self.weight_decay = WEIGHT_DECAY\n",
        "    if \"train_steps\" in flags_dic.keys():\n",
        "      self.train_steps = flags_dic[\"train_steps\"]\n",
        "    else:\n",
        "      self.train_steps = TRAIN_STEPS\n",
        "    if \"min_lr_ratio\" in flags_dic.keys():\n",
        "      self.min_lr_ratio = flags_dic[\"min_lr_ratio\"]\n",
        "    else:\n",
        "      self.min_lr_ratio = MIN_LR_RATIO\n",
        "    if \"adam_epsilon\" in flags_dic.keys():\n",
        "      self.adam_epsilon = flags_dic[\"adam_epsilon\"]\n",
        "    else:\n",
        "      self.adam_epsilon = ADAM_EPS\n",
        "    if \"num_core_per_host\" in flags_dic.keys():\n",
        "      self.num_core_per_host = flags_dic[\"num_core_per_host\"]\n",
        "    else:\n",
        "      self.num_core_per_host = NUM_CORE_PER_HOST #\n",
        "    if \"use_tpu\" in flags_dic.keys():\n",
        "      self.use_tpu = flags_dic[\"use_tpu\"]\n",
        "    else:\n",
        "      self.use_tpu = False\n",
        "    if \"clip\" in flags_dic.keys():\n",
        "      self.clip = flags_dic[\"clip\"]\n",
        "    else:\n",
        "      self.clip = CLIP\n",
        "\n",
        "# FLAGS = tf.compat.v1.flags.FLAGS\n",
        "\n",
        "# tf.compat.v1.flags.DEFINE_integer(\"warmup_steps\", WU_STEPS, \"warmup steps\")\n",
        "# tf.compat.v1.flags.DEFINE_float(\"learning_rate\", INIT_LR, \"learning rate\")\n",
        "# tf.compat.v1.flags.DEFINE_string(\"decay_method\", DECAY_METHOD, \"decay method\")\n",
        "# tf.compat.v1.flags.DEFINE_float(\"weight_decay\", WEIGHT_DECAY, \"weight decay\")\n",
        "# tf.compat.v1.flags.DEFINE_integer(\"train_steps\", TRAIN_STEPS, \"train steps\")\n",
        "# tf.compat.v1.flags.DEFINE_float(\"min_lr_ratio\", MIN_LR_RATIO, \"minimum learning rate ratio\")\n",
        "# tf.compat.v1.flags.DEFINE_float(\"adam_epsilon\", ADAM_EPS, \"adam epsilon\")\n",
        "# tf.compat.v1.flags.DEFINE_integer(\"num_core_per_host\", NUM_CORE_PER_HOST, \"number of core per host\")\n",
        "# tf.compat.v1.flags.DEFINE_bool(\"use_tpu\", False, \"whether use TPU\")\n",
        "# tf.compat.v1.flags.DEFINE_float(\"clip\", CLIP, \"gradient clipping norm\")\n",
        "\n",
        "FLAGS = Flags()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okC4RMWf6roR"
      },
      "source": [
        "loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=formula_placeholder, y_pred=y_train)\n",
        "\n",
        "train_op, learning_rate, gnorm = get_train_op(FLAGS, loss)\n",
        "\n",
        "count = y_test.get_shape()[0] # batch_size\n",
        "top1_value = tf.gather(y_test, tf.argmax(y_p, axis=-1, output_type=tf.int32), axis=1, batch_dims=1) # [batch_size, formula_length]\n",
        "top1_value = tf.slice(top1_value, [0, 1], [count, FORMULA_LENGTH])\n",
        "top1_true = tf.cast(tf.equal(formula_placeholder, top1_value), tf.int32) # [batch_size, formula_length]\n",
        "top1_accuracy = tf.reduce_sum(tf.cast(tf.equal(tf.reduce_sum(top1_true, 1), tf.fill([count], y_test.get_shape()[-1])), tf.int32)) / count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDQWJQXlAssd"
      },
      "source": [
        "init_op = tf.compat.v1.global_variables_initializer()\n",
        "\n",
        "# iterator = tf.compat.v1.data.Iterator.from_structure(tf.compat.v1.data.get_output_types(train_data),\n",
        "#                                                      tf.compat.v1.data.get_output_shapes(train_data))\n",
        "# next_element = iterator.get_next()\n",
        "# train_init_op = iterator.make_initializer(train_data)\n",
        "# test_init_op = iterator.make_initializer(test_data)\n",
        "\n",
        "train_iter = tf.compat.v1.data.make_one_shot_iterator(train_data)\n",
        "train_batch = train_iter.get_next()\n",
        "\n",
        "test_iter =  tf.compat.v1.data.make_one_shot_iterator(test_data)\n",
        "test_batch = test_iter.get_next()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PFI5zsrOI89"
      },
      "source": [
        "print(tf.compat.v1.data.get_output_types(train_data))\n",
        "print(tf.compat.v1.data.get_output_shapes(train_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0xlDoIIogJh"
      },
      "source": [
        "top1_accurs = []\n",
        "\n",
        "with tf.compat.v1.Session() as sess:\n",
        "  #Initialize variables\n",
        "  sess.run(init_op)\n",
        "\n",
        "  # Train\n",
        "  # sess.run(train_init_op)\n",
        "  for batch_index in range(NUM_BATCHES):\n",
        "    # next_elem = sess.run(next_element)\n",
        "    next_elem = sess.run(train_batch)\n",
        "    formula = next_elem['formula']\n",
        "    row_cell_context = next_elem['row_cell_context']\n",
        "    col_cell_context = next_elem['col_cell_context']\n",
        "    row_context_mask = next_elem['row_context_mask']\n",
        "    col_context_mask = next_elem['col_context_mask']\n",
        "    row_context_segment_ids = next_elem['row_context_segment_ids']\n",
        "    col_context_segment_ids = next_elem['col_context_segment_ids']\n",
        "    row_cell_indices = next_elem['row_cell_indices']\n",
        "    col_cell_indices = next_elem['col_cell_indices']\n",
        "    row_context_mask_per_cell = next_elem['row_context_mask_per_cell']\n",
        "    col_context_mask_per_cell = next_elem['col_context_mask_per_cell']\n",
        "    row_context_segment_ids_per_cell = next_elem['row_context_segment_ids_per_cell']\n",
        "    col_context_segment_ids_per_cell = next_elem['col_context_segment_ids_per_cell']\n",
        "    record_index = next_elem['record_index']\n",
        "    col_index = next_elem['col_index']\n",
        "    \n",
        "    print(formula)\n",
        "    print(len(row_cell_context))\n",
        "    print(row_cell_context)\n",
        "    print(len(col_cell_context))\n",
        "    print(col_cell_context)\n",
        "    print(len(record_index ))\n",
        "    print(record_index )\n",
        "\n",
        "    _, loss_val = sess.run([train_op, loss], feed_dict={formula_placeholder: formula,\n",
        "                                                      row_cell_context_placeholder: row_cell_context,\n",
        "                                                      col_cell_context_placeholder: col_cell_context,\n",
        "                                                      row_context_mask_placeholder: row_context_mask,\n",
        "                                                      col_context_mask_placeholder: col_context_mask,\n",
        "                                                      row_context_segment_ids_placeholder: row_context_segment_ids,\n",
        "                                                      col_context_segment_ids_placeholder: col_context_segment_ids,\n",
        "                                                      row_cell_indices_placeholder: row_cell_indices,\n",
        "                                                      col_cell_indices_placeholder: col_cell_indices,\n",
        "                                                      row_context_mask_per_cell_placeholder: row_context_mask_per_cell,\n",
        "                                                      col_context_mask_per_cell_placeholder: col_context_mask_per_cell,\n",
        "                                                      row_context_segment_ids_per_cell_placeholder: row_context_segment_ids_per_cell,\n",
        "                                                      col_context_segment_ids_per_cell_placeholder: col_context_segment_ids_per_cell,\n",
        "                                                      record_index_placeholder: record_index,\n",
        "                                                      col_index_placeholder: col_index\n",
        "                                                      })\n",
        "    print(\"batch %d: loss %f\" % (batch_index, loss_val))\n",
        "\n",
        "  # Test\n",
        "  sess.run(test_init_op)\n",
        "  while True:\n",
        "    try:\n",
        "      # next_elem = sess.run(next_element)\n",
        "      next_elem = sess.run(test_batch)\n",
        "      formula = next_elem['formula']\n",
        "      row_cell_context = next_elem['row_cell_context']\n",
        "      col_cell_context = next_elem['col_cell_context']\n",
        "      row_context_mask = next_elem['row_context_mask']\n",
        "      col_context_mask = next_elem['col_context_mask']\n",
        "      row_context_segment_ids = next_elem['row_context_segment_ids']\n",
        "      col_context_segment_ids = next_elem['col_context_segment_ids']\n",
        "      row_cell_indices = next_elem['row_cell_indices']\n",
        "      col_cell_indices = next_elem['col_cell_indices']\n",
        "      row_context_mask_per_cell = next_elem['row_context_mask_per_cell']\n",
        "      col_context_mask_per_cell = next_elem['col_context_mask_per_cell']\n",
        "      row_context_segment_ids_per_cell = next_elem['row_context_segment_ids_per_cell']\n",
        "      col_context_segment_ids_per_cell = next_elem['col_context_segment_ids_per_cell']\n",
        "      record_index = next_elem['record_index']\n",
        "      column_index = next_elem['col_index']\n",
        "\n",
        "      top1_accuracy_val = sess.run(top1_accuracy)\n",
        "      top1_accurs.append(top1_accuracy_val)\n",
        "    except tf.errors.OutOfRangeError:\n",
        "      print('End of test datset.')\n",
        "      break\n",
        "  print(\"Top1 accuracy: %.2f\" % np.mean(top1_accurs))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQrfHbQC80_N"
      },
      "source": [
        "\n",
        "# Results\n",
        "\n",
        "  <table border=\"1\">\n",
        "  <caption><i>Table 5. </i>Full model formula accuracy from paper.</caption>\n",
        "    <thead>\n",
        "      <tr>\n",
        "        <th align=left>Dataset</th>\n",
        "        <th>Top-1</th>\n",
        "        <th>Top-5</th>\n",
        "        <th>Top-10</th>\n",
        "      </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "      <tr align=center>\n",
        "        <th align=left>Enron</th>\n",
        "        <td>29.8%</td>\n",
        "        <td>41.8%</td>\n",
        "        <td>48.5%</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>Google Sheets</th>\n",
        "        <td>42.51%</td>\n",
        "        <td>54.41%</td>\n",
        "        <td>58.57%</td>\n",
        "      </tr>\n",
        "    </tbody>\n",
        "  </table>\n",
        "\n",
        "  <br/>\n",
        "\n",
        "  <table border=\"1\">\n",
        "    <caption><i>Table 6. </i>Formula accuracy on the Google Sheets test set.</caption>\n",
        "    <thead>\n",
        "      <tr>\n",
        "        <th align=left>Approach</th>\n",
        "        <th>Top-1</th>\n",
        "        <th>Top-5</th>\n",
        "        <th>Top-10</th>\n",
        "      </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "      <tr align=center>\n",
        "        <th align=left>Full model</th>\n",
        "        <td><b>42.51%</b></td>\n",
        "        <td><b>54.41%</b></td>\n",
        "        <td><b>58.57%</b></td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>— Column-based BERT</th>\n",
        "        <td>39.42%</td>\n",
        "        <td>51.68%</td>\n",
        "        <td>56.50%</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>— Row-based BERT</th>\n",
        "        <td>20.37%</td>\n",
        "        <td>40.87%</td>\n",
        "        <td>48.37%</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>— Convolution layers</th>\n",
        "        <td>38.43%</td>\n",
        "        <td>51.31%</td>\n",
        "        <td>55.87%</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>— Two-stage decoding</th>\n",
        "        <td>41.12%</td>\n",
        "        <td>53.57%</td>\n",
        "        <td>57.95%</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>— Pretraining</th>\n",
        "        <td>31.51%</td>\n",
        "        <td>42.64%</td>\n",
        "        <td>49.77%</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>Row-based RobustFill</th>\n",
        "        <td>31.14%</td>\n",
        "        <td>40.09%</td>\n",
        "        <td>47.10%</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>Column-based RobustFill</th>\n",
        "        <td>20.65%</td>\n",
        "        <td>39.69%</td>\n",
        "        <td>46.96%</td>\n",
        "      </tr>\n",
        "      <tr align=center>\n",
        "        <th align=left>No context</th>\n",
        "        <td>10.56%</td>\n",
        "        <td>23.27%</td>\n",
        "        <td>31.96%</td>\n",
        "      </tr>\n",
        "    </tbody>\n",
        "  </table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy2gRgxe813S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}